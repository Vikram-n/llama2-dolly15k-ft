{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  6 19:46:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   38C    P8    16W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, PeftModel, LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration databricks--databricks-dolly-15k-7427aa6e57c34282\n",
      "Reusing dataset json (/root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e47a891094d4e0c8059c04a1f9a331d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data , test_data =load_dataset(\"databricks/databricks-dolly-15k\", split=[\"train[:80%]\", \"train[80%:]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"context\"].str.len()>=10]\n",
    "test_df = test_df[test_df[\"context\"].str.len()>=10]\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "test_df.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If I have more pieces at the time of stalemate...</td>\n",
       "      <td>Stalemate is a situation in chess where the pl...</td>\n",
       "      <td>No. \\nStalemate is a drawn position. It doesn'...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given a reference text about Lollapalooza, whe...</td>\n",
       "      <td>Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...</td>\n",
       "      <td>Lollapalooze is an annual musical festival hel...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who was John Moses Browning?</td>\n",
       "      <td>John Moses Browning (January 23, 1855 – Novemb...</td>\n",
       "      <td>John Moses Browning is one of the most well-kn...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>What languages are spoken in Tunisia?</td>\n",
       "      <td>The official language of Tunisia is Modern Sta...</td>\n",
       "      <td>The official language of Tunisia is Modern Sta...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>Extract the venues of each Phish concert refer...</td>\n",
       "      <td>On October 1, 2008, the band announced on thei...</td>\n",
       "      <td>-Hampton Coliseum\\n-Fenway Park\\n-Red Rocks Am...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>Extract the title of the game, the name of its...</td>\n",
       "      <td>Horizon Zero Dawn is a 2017 action role-playin...</td>\n",
       "      <td>Horizon Zero Dawn, Guerrilla Games, Aloy</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>What's the architecture in Maskavas Forstate l...</td>\n",
       "      <td>Maskavas Forštate (German: Moskauer Vorstadt) ...</td>\n",
       "      <td>The architecture of Maskavas Forštate reflects...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>From the passage provided, extract the union t...</td>\n",
       "      <td>South India, also known as Peninsular India, c...</td>\n",
       "      <td>Andaman and Nicobar Islands, Lakshadweep and P...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3565 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  \\\n",
       "0            When did Virgin Australia start operating?   \n",
       "1                       When was Tomoaki Komorida born?   \n",
       "2     If I have more pieces at the time of stalemate...   \n",
       "3     Given a reference text about Lollapalooza, whe...   \n",
       "4                          Who was John Moses Browning?   \n",
       "...                                                 ...   \n",
       "3560              What languages are spoken in Tunisia?   \n",
       "3561  Extract the venues of each Phish concert refer...   \n",
       "3562  Extract the title of the game, the name of its...   \n",
       "3563  What's the architecture in Maskavas Forstate l...   \n",
       "3564  From the passage provided, extract the union t...   \n",
       "\n",
       "                                                context  \\\n",
       "0     Virgin Australia, the trading name of Virgin A...   \n",
       "1     Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "2     Stalemate is a situation in chess where the pl...   \n",
       "3     Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...   \n",
       "4     John Moses Browning (January 23, 1855 – Novemb...   \n",
       "...                                                 ...   \n",
       "3560  The official language of Tunisia is Modern Sta...   \n",
       "3561  On October 1, 2008, the band announced on thei...   \n",
       "3562  Horizon Zero Dawn is a 2017 action role-playin...   \n",
       "3563  Maskavas Forštate (German: Moskauer Vorstadt) ...   \n",
       "3564  South India, also known as Peninsular India, c...   \n",
       "\n",
       "                                               response  \\\n",
       "0     Virgin Australia commenced services on 31 Augu...   \n",
       "1            Tomoaki Komorida was born on July 10,1981.   \n",
       "2     No. \\nStalemate is a drawn position. It doesn'...   \n",
       "3     Lollapalooze is an annual musical festival hel...   \n",
       "4     John Moses Browning is one of the most well-kn...   \n",
       "...                                                 ...   \n",
       "3560  The official language of Tunisia is Modern Sta...   \n",
       "3561  -Hampton Coliseum\\n-Fenway Park\\n-Red Rocks Am...   \n",
       "3562           Horizon Zero Dawn, Guerrilla Games, Aloy   \n",
       "3563  The architecture of Maskavas Forštate reflects...   \n",
       "3564  Andaman and Nicobar Islands, Lakshadweep and P...   \n",
       "\n",
       "                    category  \n",
       "0                  closed_qa  \n",
       "1                  closed_qa  \n",
       "2     information_extraction  \n",
       "3                  closed_qa  \n",
       "4     information_extraction  \n",
       "...                      ...  \n",
       "3560               closed_qa  \n",
       "3561  information_extraction  \n",
       "3562  information_extraction  \n",
       "3563               closed_qa  \n",
       "3564  information_extraction  \n",
       "\n",
       "[3565 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3565, 4), (901, 4))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, split=\"train\"):\n",
    "    text_col=[]\n",
    "    instruction=\"\"\"write a precise summary of the below input text. \n",
    "    Return your response in bullet points which covers the keypoints of the input text.\n",
    "    only provide full sentences response summary.\"\"\"\n",
    "    if split == \"train\":\n",
    "        for _, row in df.iterrows():\n",
    "            inst=row[\"instruction\"]\n",
    "            inputc=row[\"context\"]\n",
    "            output=row[\"response\"]\n",
    "            text = (\"### Instruction: \\n\" + instruction +\"\\n\" + inst\n",
    "                    +\"\\n### Input: \\n\" + inputc + \"\\n### Response: \\n\" + output)\n",
    "            text_col.append(text)\n",
    "        df.loc[:, \"text\"] = text_col\n",
    "    else:\n",
    "        for _, row in df.iterrows():\n",
    "            inst=row[\"instruction\"]\n",
    "            inputc=row[\"context\"]\n",
    "            text = (\"### Instruction: \\n\" + instruction +\"\\n\" + inst\n",
    "                    +\"\\n### Input: \\n\" + inputc + \"\\n### Response: \\n\" )\n",
    "            text_col.append(text)\n",
    "        df.loc[:, \"text\"] = text_col\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_dataset(train_df, \"train\")\n",
    "test_df = prepare_dataset(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "write a precise summary of the below input text. \n",
      "    Return your response in bullet points which covers the keypoints of the input text.\n",
      "    only provide full sentences response summary.\n",
      "When did Virgin Australia start operating?\n",
      "### Input: \n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "### Response: \n",
      "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "write a precise summary of the below input text. \n",
      "    Return your response in bullet points which covers the keypoints of the input text.\n",
      "    only provide full sentences response summary.\n",
      "Using the two paragraphs below, when was the Ukrainian Chorus Dumka of NY founded, and when did it play in Ukraine for the first time?\n",
      "### Input: \n",
      "Ukrainian Chorus Dumka of New York was founded in 1949 with the goal \"to preserve and cultivate the rich musical heritage of Ukraine\", both for the church and for secular occasions. In the beginning, the chorus was a men's chorus of Ukrainian immigrants who met to sing music they loved. The first music director was L. Krushelnycky. The group became a mixed choir in 1959. \n",
      "\n",
      "They have performed in New York at locations including in Alice Tully Hall, Avery Fisher Hall, Brooklyn Academy of Music, Carnegie Hall, Madison Square Garden, St. Patrick's Cathedral, and Town Hall. They toured to the Kennedy Center in Washington, and in several European capitals. In 1990, the chorus toured Ukraine for the first time, singing in Kyiv, Lviv, Poltava, and Kaniv. They made recordings of both church and secular music.\n",
      "### Response: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category', 'text'],\n",
       "    num_rows: 3565\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c442bb7fe68a4391930c4539bc963f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_type=\"float16\"\n",
    ")\n",
    "model =AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config,trust_remote_code=True,device_map=\"auto\")\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,return_token_type_ids=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lora configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training args & prepare model for kbit training\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./llama2_ft_dolly\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345e48ed3eb24098b93f027911d93eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikram-n\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230906_194641-13fvr77i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vikram-n/huggingface/runs/13fvr77i\" target=\"_blank\">amber-haze-5</a></strong> to <a href=\"https://wandb.ai/vikram-n/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 11:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.156600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training: 723.2419702569023\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "trainer.train()\n",
    "end_time = perf_counter()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Time taken for training: {training_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model\n",
    "model_to_save.save_pretrained(\"./llama2_ft_dolly/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(\"./llama2_ft_dolly/results\")\n",
    "tmodel = get_peft_model(model_to_save, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "write a precise summary of the below input text. \n",
      "    Return your response in bullet points which covers the keypoints of the input text.\n",
      "    only provide full sentences response summary.\n",
      "When was Irina Vysheslavska born?\n",
      "### Input: \n",
      "Irina Vysheslavska was born in Kiev on February 20, 1939, into a family of great cultural traditions. Her father Leonid Vysheslavsky was a noted poet and her mother Agnes Baltaga was a writer. Several of her ancestors were priests in Greece, Romania and Ukraine.\n",
      "### Response: \n",
      "- Irina Visheshlavsa is from kiev ukraine and she has many famous relatives such as her dad leonard visheshlavsa who wrote poetry and her mum agnes baltage who also wrote poems. they are both very talented people but unfortunately irinas parents died when she was young so now shes an orphan living with her uncle.\n",
      "\n",
      "Time taken for output: 23.099115974036977 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "start_time = perf_counter()\n",
    "text = test_df[\"text\"][5]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.6, do_sample=True, top_k=5, temperature=0.5, repetition_penalty=1.2)\n",
    "outputs = tmodel.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    generation_config=generation_config,\n",
    "    max_new_tokens=100,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "end_time = perf_counter()\n",
    "output_time = end_time - start_time\n",
    "print(f\"Time taken for output: {output_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "write a precise summary of the below input text. \n",
      "    Return your response in bullet points which covers the keypoints of the input text.\n",
      "    only provide full sentences response summary.\n",
      "Was Venice always part of Italy?\n",
      "### Input: \n",
      "The Republic of Venice lost its independence when Napoleon Bonaparte conquered Venice on 12 May 1797 during the War of the First Coalition. Napoleon was seen as something of a liberator by the city's Jewish population. He removed the gates of the Ghetto and ended the restrictions on when and where Jews could live and travel in the city.\n",
      "\n",
      "Venice became Austrian territory when Napoleon signed the Treaty of Campo Formio on 12 October 1797. The Austrians took control of the city on 18 January 1798. Venice was taken from Austria by the Treaty of Pressburg in 1805 and became part of Napoleon's Kingdom of Italy. It was returned to Austria following Napoleon's defeat in 1814, when it became part of the Austrian-held Kingdom of Lombardy–Venetia. In 1848 a revolt briefly re-established the Venetian republic under Daniele Manin, but this was crushed in 1849. In 1866, after the Third Italian War of Independence, Venice, along with the rest of the Veneto, became part of the newly created Kingdom of Italy.\n",
      "### Response: \n",
      "Venice is an ancient city located in northeastern Italy at the northern end of the Adriatic Sea that has been known for centuries as one of Europe’s most beautiful cities. The lagoon city is famous for its canals, bridges, palaces, churches, squares and many other attractions that attract millions of tourists each year.\n",
      "\n",
      "Venice was founded around AD 421, although some scholars believe there may have been settlements\n",
      "Time taken for output: 67.75193372997455 seconds\n"
     ]
    }
   ],
   "source": [
    "text1 = test_df[\"text\"][10]\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\").to(\"cuda\")\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.6, do_sample=True, top_k=5, temperature=0.5, repetition_penalty=1.2)\n",
    "outputs = tmodel.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    generation_config=generation_config,\n",
    "    max_new_tokens=100,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "end_time = perf_counter()\n",
    "output_time = end_time - start_time\n",
    "print(f\"Time taken for output: {output_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  6 19:59:51 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 58%   77C    P2   183W / 300W |   8593MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9454b29b9f6f44fdab256a965f03b50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a035a4aaf4456595e7fb9a2d83e13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2f9403db194d758d462310791206a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e96acb9811c48cc9621919eea6f9699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f147d8973c2a47b69b088853ce9fd966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca83b119b23d4c7b83725471aa0af339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7e6c7f6e5e4d79a4da95bae44803db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1648cec94cd84709900bbf8d98bfc3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138c2bdeb3984ef89b7d9d48e2cde0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb3839ae73a4380b3f47c156c22ef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edb205094174f73b0738d5a5ad3e66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eda600e60c44dba2b801b5609fc69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3874fcd40f4b4781893384ae1620e70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e320515cd1041d7b31ce7a8b5e78f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b5d8daa7a84d0cad7e2aad96593622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57592554130545d2b36c25662494d8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f3a7248e9470da5ab4d758072d0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef2e3fc097647e2ac82afb8a5a09658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce4057afe5f43ff83babb577e384872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85946905390455bb6137a2d40b0944b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d84a231eff43649e2ac33bac732fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860c432329ac427bac6afb97d8a05650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e204a6a6a39d426eaa41cf8398e47e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfc0033edc34a83aacbc43d9f10149d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d715e79cda147dd945b81302ea289e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e305eb4c0145e7b08698842af069f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0577cf95ea914fa2a276a925767fd4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f6af9edb064882b51f1ceb58141716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a29d1c144ad43eabe4d1d876e45831e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874ee70634d2429c9ebf89154e89d8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35ba4d5c2e848e184ed06dd2c2b2c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292d2ec834234dec938bf8ad2a706d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006c8cc3f33542c19e58c958e92c6198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1baf118bc294a9ba7c49e505532b1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPTQConfig\n",
    "\n",
    "quantization_config = GPTQConfig(bits=4,dataset=[\"c4\"],desc_act=False)\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\"./llama2_ft_dolly/outputs\", quantization_config=quantization_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama2_ft_dolly/quantized/tokenizer_config.json',\n",
       " './llama2_ft_dolly/quantized/special_tokens_map.json',\n",
       " './llama2_ft_dolly/quantized/tokenizer.model',\n",
       " './llama2_ft_dolly/quantized/added_tokens.json',\n",
       " './llama2_ft_dolly/quantized/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the quantized model\n",
    "quant_model.save_pretrained(\"./llama2_ft_dolly/quantized\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./llama2_ft_dolly/quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "write a precise summary of the below input text. \n",
      "    Return your response in bullet points which covers the keypoints of the input text.\n",
      "    only provide full sentences response summary.\n",
      "When was Irina Vysheslavska born?\n",
      "### Input: \n",
      "Irina Vysheslavska was born in Kiev on February 20, 1939, into a family of great cultural traditions. Her father Leonid Vysheslavsky was a noted poet and her mother Agnes Baltaga was a writer. Several of her ancestors were priests in Greece, Romania and Ukraine.\n",
      "### Response: \n",
      "Irina Vyscheslawskaya (b. Feb 20th) is Ukrainian composer from Kyiv. She studied composition at the Kyiv Conservatory under Mykola Kolomiyets with further studies at the Moscow State Tchaikovsky Conservatory as well as private lessons in Paris with Nadia Boulanger. From an early age she composed music for piano and voice that has been performed by leading artists including Martha Argerich, Ol\n",
      "Time taken for output: 3.429542585974559 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "test = test_df[\"text\"][5]\n",
    "inputs = tokenizer(test, return_tensors=\"pt\").to(\"cuda\")\n",
    "gen_config = GenerationConfig(\n",
    "    penalty_alpha=0.6, do_sample=True, top_k=5, temperature=0.5, repetition_penalty=1.2)\n",
    "outputs = quant_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    generation_config=gen_config,\n",
    "    max_new_tokens=100,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "end_time = perf_counter()\n",
    "output_time = end_time - start_time\n",
    "print(f\"Time taken for output: {output_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "write a precise summary of the below input text. \n",
      "    Return your response in bullet points which covers the keypoints of the input text.\n",
      "    only provide full sentences response summary.\n",
      "Was Venice always part of Italy?\n",
      "### Input: \n",
      "The Republic of Venice lost its independence when Napoleon Bonaparte conquered Venice on 12 May 1797 during the War of the First Coalition. Napoleon was seen as something of a liberator by the city's Jewish population. He removed the gates of the Ghetto and ended the restrictions on when and where Jews could live and travel in the city.\n",
      "\n",
      "Venice became Austrian territory when Napoleon signed the Treaty of Campo Formio on 12 October 1797. The Austrians took control of the city on 18 January 1798. Venice was taken from Austria by the Treaty of Pressburg in 1805 and became part of Napoleon's Kingdom of Italy. It was returned to Austria following Napoleon's defeat in 1814, when it became part of the Austrian-held Kingdom of Lombardy–Venetia. In 1848 a revolt briefly re-established the Venetian republic under Daniele Manin, but this was crushed in 1849. In 1866, after the Third Italian War of Independence, Venice, along with the rest of the Veneto, became part of the newly created Kingdom of Italy.\n",
      "### Response: \n",
      "No, Venice has never been part of Italy. After losing its independence due to French invasion (the Napoleonic wars), Venice was annexed into the Austro-Hungarian empire for over a century before being ceded back to Italy at the end of World War I.\n",
      "\n",
      "During that time period, there were multiple attempts made to establish a new independent nation called \"Italy\" out of various parts of central Europe. However, these efforts ultimately failed because they\n",
      "Time taken for output: 3.459266507998109 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "test1 = test_df[\"text\"][10]\n",
    "inputs = tokenizer(test1, return_tensors=\"pt\").to(\"cuda\")\n",
    "gen_config = GenerationConfig(\n",
    "    penalty_alpha=0.6, do_sample=True, top_k=5, temperature=0.5, repetition_penalty=1.2)\n",
    "outputs = quant_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    generation_config=gen_config,\n",
    "    max_new_tokens=100,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "end_time = perf_counter()\n",
    "output_time = end_time - start_time\n",
    "print(f\"Time taken for output: {output_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
